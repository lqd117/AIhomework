import java.lang.reflect.Array;
import java.lang.reflect.Parameter;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.Map;

import org.apache.commons.math3.analysis.function.Multiply;
import org.apache.commons.math3.exception.ZeroException;

import MyTuple.ThreeTuple;
import MyTuple.TwoTuple;

public class MyNeuralNetwork {
	
	private MyNumPy np = new MyNumPy();
	
	public Map<String, MyArray> initialize_parameters_deep(int[] layer_dims){
		/**
		 * Arguments:
		 * layer_dims -- array (list) containing the dimensions of each layer in our network
		 * 
		 * Returns:
		 * parameters --  dictionary containing your parameters "W1", "b1", ..., "WL", "bL":
		 * 		Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])
		 * 		bl -- bias vector of shape (layer_dims[l], 1)
		 */
		Map<String, MyArray> parameters = new HashMap<>();
		int L = layer_dims.length;
		for(int i=1;i<L;i++){
			parameters.put("W"+String.valueOf(i), np.random.randn(layer_dims[i],layer_dims[i-1]).mult(Math.sqrt(2.0/(double)layer_dims[i-1])));
			parameters.put("b"+String.valueOf(i), np.zeros(layer_dims[i],1));
		}
		return parameters;
	}

	public TwoTuple<MyArray, ThreeTuple<MyArray, MyArray, MyArray>> 
									linear_forward(MyArray A,MyArray W,MyArray b){
		
		/**
		 * Implement the linear part of a layer's forward propagation.
		 * Arguments:
		 * A -- activations from previous layer (size of previous layer, number of examples)
		 * W -- weights matrix(size of current layer, size of previous layer)
		 * b -- bias vector (size of the current layer, 1)
		 * 
		 * Returns:
		 * Z -- the input of the activation function, also called pre-activation parameter 
		 * cache -- a dictionary containing "A", "W" and "b" ; stored for computing the backward pass efficiently
		 */
		MyArray Z = np.dot(W,A).add(b);
		ThreeTuple<MyArray, MyArray, MyArray> cache = 
				new ThreeTuple<MyArray, MyArray, MyArray>(A, W, b);
		TwoTuple<MyArray, ThreeTuple<MyArray, MyArray, MyArray>> twoTuple = 
				new TwoTuple<MyArray, ThreeTuple<MyArray,MyArray,MyArray>>(Z, cache);
		
		return twoTuple;
	}

	public TwoTuple<MyArray, TwoTuple<ThreeTuple<MyArray, MyArray, MyArray>, MyArray>>
									linear_activation_forward(MyArray A_prev,MyArray W,
											MyArray b,String activation){
		/**
		 * Implement the forward propagation for the LINEAR->ACTIVATION layer
		 * 
		 * Arguments:
		 * A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)
		 * W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)
		 * b -- bias vector, numpy array of shape (size of the current layer, 1)
		 * activation -- the activation to be used in this layer, stored as a text string: "sigmoid" or "relu"
		 * 
		 * Returns:
		 * A -- the output of the activation function, also called the post-activation value 
		 * cache -- a dictionary containing "linear_cache" and "activation_cache";
             stored for computing the backward pass efficiently
		 */
		MyArray Z,A = null,activation_cache = null;
		ThreeTuple<MyArray, MyArray, MyArray> linear_cache = null;
		if(activation == "sigmoid"){
			TwoTuple<MyArray, ThreeTuple<MyArray, MyArray, MyArray>> twoTuple =
					linear_forward(A_prev, W, b);
			Z = twoTuple.getFirst();
			linear_cache = twoTuple.getSecond();
			TwoTuple<MyArray, MyArray> twoTuple2 = sigmoid(Z);
			A = twoTuple2.getFirst();
			activation_cache = twoTuple2.getSecond();
		}else if(activation == "relu"){
			TwoTuple<MyArray, ThreeTuple<MyArray, MyArray, MyArray>> twoTuple =
					linear_forward(A_prev, W, b);
			Z = twoTuple.getFirst();
			linear_cache = twoTuple.getSecond();
			TwoTuple<MyArray, MyArray> twoTuple2 = relu(Z);
			A = twoTuple2.getFirst();
			activation_cache = twoTuple2.getSecond();
		}
		TwoTuple<ThreeTuple<MyArray, MyArray, MyArray>, MyArray> cache = 
				new TwoTuple<ThreeTuple<MyArray,MyArray,MyArray>, MyArray>(linear_cache, activation_cache);
		TwoTuple<MyArray, TwoTuple<ThreeTuple<MyArray, MyArray, MyArray>, MyArray>> twoTuple =
				new TwoTuple<MyArray, TwoTuple<ThreeTuple<MyArray, MyArray, MyArray>, MyArray>>(A,cache);
		return twoTuple;
	}
	
	public TwoTuple<MyArray, ArrayList<TwoTuple<ThreeTuple<MyArray, MyArray, MyArray>, MyArray>>>
									L_model_forward(MyArray X, Map<String, MyArray> parameters) {
		/**
		 * Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation
		 * 
		 * Arguments:
		 * X -- data, numpy array of shape (input size, number of examples)
		 * parameters -- output of initialize_parameters_deep()
		 * 
		 * Returns:
		 * AL -- last post-activation value
		 * caches -- list of caches containing:
                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)
		 */
		ArrayList<TwoTuple<ThreeTuple<MyArray, MyArray, MyArray>, MyArray>> caches =
				new ArrayList<>();
		MyArray A = X,A_prev,AL;
		TwoTuple<ThreeTuple<MyArray, MyArray, MyArray>, MyArray> cache;
		TwoTuple<MyArray, TwoTuple<ThreeTuple<MyArray, MyArray, MyArray>, MyArray>> twoTuple;
		int L = parameters.size()/2;
		for(int i=1;i<=L;i++){
			A_prev = new MyArray(A.getRows(), A.getColumns());
			for(int i1=0;i1<A_prev.getRows();i1++){
				for(int j1=0;j1<A_prev.getColumns();j1++){
					A_prev.set(i1, j1, A.getNumber(i1, j1));
				}
			}
			twoTuple = linear_activation_forward(
					A_prev, parameters.get("W"+String.valueOf(i)), parameters.get("b"+String.valueOf(i)), "relu");
			A = twoTuple.getFirst();
			cache = twoTuple.getSecond();
			caches.add(cache);
		}
//		twoTuple = linear_activation_forward(
//				A, parameters.get("W"+String.valueOf(L)), parameters.get("b"+String.valueOf(L)), "relu");
//		AL = twoTuple.getFirst();
//		cache = twoTuple.getSecond();
//		caches.add(cache);
		TwoTuple<MyArray, ArrayList<TwoTuple<ThreeTuple<MyArray, MyArray, MyArray>, MyArray>>> result =
				new TwoTuple<MyArray, ArrayList<TwoTuple<ThreeTuple<MyArray,MyArray,MyArray>,MyArray>>>(A, caches);
		return result;
	}
	
	public double compute_cost(MyArray AL,MyArray Y){
		/**
		 * Implement the cost function defined by equation (7).
		 * 
		 * Arguments:
		 * AL -- probability vector corresponding to your label predictions, shape (1, number of examples)
		 * Y -- true "label" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)
		 * 
		 * Returns:
		 * cost -- cross-entropy cost
		 */
		int m = Y.getColumns();
		double cost=0;
		for(int i=0;i<AL.getColumns();i++){
			cost += (AL.getNumber(0, i) - Y.getNumber(0, i)) * (AL.getNumber(0, i) - Y.getNumber(0, i));
		}
		cost = cost/(2*m);
//		MyArray temp = np.dot(np.log(AL), Y.getT()).add(np.dot(np.log(AL.getOpposite().add(1)), Y.getOpposite().add(1).getT()));
//		
//		for(int i=0;i<temp.getRows();i++){
//			for(int j=0;j<temp.getColumns();j++){
//				cost += temp.getNumber(i, j);
//			}
//		}
//		cost = -1.0/(double)m * cost;
		return cost;
	}
	
	public ThreeTuple<MyArray, MyArray, MyArray> 
									linear_backward(MyArray dZ,ThreeTuple<MyArray, MyArray, MyArray> cache){
		/**
		 * Implement the linear portion of backward propagation for a single layer (layer l)
		 * 
		 * Arguments:
		 * dZ -- Gradient of the cost with respect to the linear output (of current layer l)
		 * cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer
		 * 
		 * Returns:
		 * dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
		 * dW -- Gradient of the cost with respect to W (current layer l), same shape as W
		 * db -- Gradient of the cost with respect to b (current layer l), same shape as b
		 */
		MyArray A_prev,W,b;
		A_prev = cache.getFirst();W = cache.getSecond();b = cache.getThird();
		int m = A_prev.getColumns();
		MyArray dW = np.dot(dZ, A_prev.getT()).mult(1/(double)m);
		MyArray db = np.sum(dZ,1).mult(1/(double)m);
		MyArray dA_prev = np.dot(W.getT(), dZ);
		ThreeTuple<MyArray, MyArray, MyArray> threeTuple = new ThreeTuple<MyArray, MyArray, MyArray>(dA_prev, dW, db);
		
		return threeTuple;
	}

	public ThreeTuple<MyArray, MyArray, MyArray> linear_activation_backward(MyArray dA,
			TwoTuple<ThreeTuple<MyArray, MyArray, MyArray>, MyArray> cache,String activation){
		/**
		 * Implement the backward propagation for the LINEAR->ACTIVATION layer.
		 * 
		 * Arguments:
		 * dA -- post-activation gradient for current layer l 
		 * cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently
		 * activation -- the activation to be used in this layer, stored as a text string: "sigmoid" or "relu"
		 * 
		 * Returns:
		 * dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
		 * dW -- Gradient of the cost with respect to W (current layer l), same shape as W
		 * db -- Gradient of the cost with respect to b (current layer l), same shape as b
		 */
		ThreeTuple<MyArray, MyArray, MyArray> linear_cache = cache.getFirst();
		ThreeTuple<MyArray, MyArray, MyArray> threeTuple;
		MyArray activation_cache = cache.getSecond(),dA_prev = null,dW = null,db = null;

		if(activation == "relu"){
			MyArray dZ = relu_backward(dA, activation_cache);
			threeTuple = linear_backward(dZ, linear_cache);
			dA_prev = threeTuple.getFirst();
			dW = threeTuple.getSecond();
			db = threeTuple.getThird();
		}else if(activation == "sigmoid"){
			MyArray dZ = sigmoid_backward(dA, activation_cache);
			threeTuple = linear_backward(dZ, linear_cache);
			dA_prev = threeTuple.getFirst();
			dW = threeTuple.getSecond();
			db = threeTuple.getThird();
		}
		ThreeTuple<MyArray, MyArray, MyArray> result = new ThreeTuple<MyArray, MyArray, MyArray>(dA_prev, dW, db);
		return result;
	}
	
	
	public Map<String, MyArray> L_model_backward(
			MyArray AL,MyArray Y,ArrayList<TwoTuple<ThreeTuple<MyArray, MyArray, MyArray>, MyArray>> caches){
		/**
		 * Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group
		 * 
		 * Arguments:
		 * AL -- probability vector, output of the forward propagation (L_model_forward())
		 * Y -- true "label" vector (containing 0 if non-cat, 1 if cat)
		 * caches -- list of caches containing:
         *       every cache of linear_activation_forward() with "relu" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)
         *       the cache of linear_activation_forward() with "sigmoid" (it's caches[L-1])
         * 
         * Returns:
         * grads -- A dictionary with the gradients
         *    grads["dA" + str(l)] = ... 
         *    grads["dW" + str(l)] = ...
         *    grads["db" + str(l)] = ... 
		 */
		
		Map<String, MyArray> grads = new HashMap<>();
		int L = caches.size();
		int m = AL.getColumns();
		
		//MyArray dAL = np.divide(Y,AL).add(np.divide(Y.getOpposite().add(1), AL.getOpposite().add(1)).getOpposite()).getOpposite();
		
		MyArray dAL = AL.add(Y.getOpposite());
		TwoTuple<ThreeTuple<MyArray, MyArray, MyArray>, MyArray> current_cache = caches.get(L-1);
		ThreeTuple<MyArray, MyArray, MyArray> threeTuple = linear_activation_backward(dAL, current_cache, "relu");
		grads.put("dA"+String.valueOf(L-1), threeTuple.getFirst());
		grads.put("dW"+String.valueOf(L), threeTuple.getSecond());
		grads.put("db"+String.valueOf(L), threeTuple.getThird());
		
		for(int i=L-2;i>=0;i--){
			current_cache = caches.get(i);
			threeTuple = linear_activation_backward(grads.get("dA"+String.valueOf(i+1)), current_cache, "relu");
			grads.put("dA"+String.valueOf(i), threeTuple.getFirst());
			grads.put("dW"+String.valueOf(i+1), threeTuple.getSecond());
			grads.put("db"+String.valueOf(i+1), threeTuple.getThird());
		}
		
		return grads;
	}
	
	public Map<String, MyArray> update_parameters(
			Map<String, MyArray> parameters,Map<String, MyArray> grads,double learning_rate ){
		/**
		 * Update parameters using gradient descent
		 * 
		 * Arguments:
		 * parameters -- python dictionary containing your parameters '
		 * grads -- python dictionary containing your gradients, output of L_model_backward
		 * 
		 * Returns:
		 * parameters -- python dictionary containing your updated parameters 
         *         parameters["W" + str(l)] = ... 
         *         parameters["b" + str(l)] = ...
		 */
		int L = parameters.size()/2;
		for(int i=0;i<L;i++){
			parameters.put("W"+String.valueOf(i+1), 
					parameters.get("W"+String.valueOf(i+1)).add(
							grads.get("dW"+String.valueOf(i+1)).mult(learning_rate).getOpposite()));
			parameters.put("b"+String.valueOf(i+1),
					parameters.get("b"+String.valueOf(i+1)).add(
							grads.get("db"+String.valueOf(i+1)).mult(learning_rate).getOpposite()));
		}
		return parameters;
	}
	
	public TwoTuple<MyArray, MyArray> sigmoid(MyArray Z){
		/**
		 * Implements the sigmoid activation in numpy
		 * 
		 * Arguments:
		 * Z -- numpy array of any shape
		 * 
		 * Returns:
		 * A -- output of sigmoid(z), same shape as Z
		 * cache -- returns Z as well, useful during backpropagation 
		 */
		MyArray A = new MyArray(Z.getRows(), Z.getColumns());
		MyArray cache = new MyArray(Z.getRows(), Z.getColumns());
		for(int i=0;i<A.getRows();i++){
			for(int j=0;j<A.getColumns();j++){
				A.set(i, j, 1/(1+Math.exp(-Z.getNumber(i, j))));
				cache.set(i, j, Z.getNumber(i, j));
			}
		}
		TwoTuple<MyArray, MyArray> twoTuple = new TwoTuple<MyArray, MyArray>(A, cache);
		return twoTuple;
	}

	public TwoTuple<MyArray, MyArray> relu(MyArray Z){
		/**
		 * Implement the RELU function.
		 * 
		 * Arguments:
		 * Z -- Output of the linear layer, of any shape
		 * 
		 * Returns:
		 * A -- Post-activation parameter, of the same shape as Z
		 * cache -- a dictionary containing "A" ; stored for computing the backward pass efficiently
		 */
		MyArray A = new MyArray(Z.getRows(), Z.getColumns());
		MyArray cache = new MyArray(Z.getRows(), Z.getColumns());
		for(int i=0;i<A.getRows();i++){
			for(int j=0;j<A.getColumns();j++){
				A.set(i, j, Math.max(0, Z.getNumber(i, j)));
				cache.set(i, j, Z.getNumber(i, j));
			}
		}
		TwoTuple<MyArray, MyArray> twoTuple = new TwoTuple<MyArray, MyArray>(A, cache);
		return twoTuple;
	}

	public MyArray relu_backward(MyArray dA,MyArray cache){
		/**
		 * Implement the backward propagation for a single RELU unit.
		 * 
		 * Arguments:
		 * dA -- post-activation gradient, of any shape
		 * cache -- 'Z' where we store for computing backward propagation efficiently
		 * 
		 * Returns:
		 * dZ -- Gradient of the cost with respect to Z
		 */
		MyArray Z = cache;
		MyArray dZ = new MyArray(dA.getRows(), dA.getColumns());
		for(int i=0;i<dZ.getRows();i++){
			for(int j=0;j<dZ.getColumns();j++){
				if(Z.getNumber(i, j) <= 0){
					dZ.set(i, j, 0);
				}else{
					dZ.set(i, j, dA.getNumber(i, j));
				}
			}
		}
		return dZ;
	}

	public MyArray sigmoid_backward(MyArray dA,MyArray cache){
		/**
		 * Implement the backward propagation for a single SIGMOID unit.
		 * 
		 * Arguments:
		 * dA -- post-activation gradient, of any shape
		 * cache -- 'Z' where we store for computing backward propagation efficiently
		 * 
		 * Returns:
		 * dZ -- Gradient of the cost with respect to Z
		 */
		
		MyArray Z = cache;
		MyArray s = new MyArray(Z.getRows(), Z.getColumns());
		MyArray dZ = new MyArray(Z.getRows(), Z.getColumns());
		for(int i=0;i<s.getRows();i++){
			for(int j=0;j<s.getColumns();j++){
				s.set(i, j, 1/(1+Math.exp(-Z.getNumber(i, j))));
				dZ.set(i, j, dA.getNumber(i, j)*s.getNumber(i, j)*(1-s.getNumber(i, j)));
			}
		}
		return dZ;
	}
}
